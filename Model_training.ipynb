{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hickle in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from hickle) (0.3.1.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from hickle) (1.16.4)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from hickle) (2.8.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from h5py->hickle) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install hickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-image in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (0.14.5)\n",
      "Requirement not upgraded as not directly required: networkx>=1.8 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from scikit-image) (2.1)\n",
      "Requirement not upgraded as not directly required: cloudpickle>=0.2.1 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from scikit-image) (0.5.3)\n",
      "Requirement not upgraded as not directly required: six>=1.10.0 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from scikit-image) (1.11.0)\n",
      "Requirement not upgraded as not directly required: pillow>=4.3.0 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from scikit-image) (5.4.1)\n",
      "Requirement not upgraded as not directly required: matplotlib>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from scikit-image) (2.2.2)\n",
      "Requirement not upgraded as not directly required: scipy>=0.17.0 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from scikit-image) (1.2.1)\n",
      "Requirement not upgraded as not directly required: PyWavelets>=0.4.0 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from scikit-image) (0.5.2)\n",
      "Requirement not upgraded as not directly required: decorator>=4.1.0 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from networkx>=1.8->scikit-image) (4.3.0)\n",
      "Requirement not upgraded as not directly required: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from matplotlib>=2.0.0->scikit-image) (2.7.3)\n",
      "Requirement not upgraded as not directly required: subprocess32 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from matplotlib>=2.0.0->scikit-image) (3.5.0)\n",
      "Requirement not upgraded as not directly required: cycler>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from matplotlib>=2.0.0->scikit-image) (0.10.0)\n",
      "Requirement not upgraded as not directly required: backports.functools-lru-cache in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from matplotlib>=2.0.0->scikit-image) (1.5)\n",
      "Requirement not upgraded as not directly required: pytz in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from matplotlib>=2.0.0->scikit-image) (2018.4)\n",
      "Requirement not upgraded as not directly required: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from matplotlib>=2.0.0->scikit-image) (2.2.0)\n",
      "Requirement not upgraded as not directly required: numpy>=1.7.1 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from matplotlib>=2.0.0->scikit-image) (1.16.4)\n",
      "Requirement not upgraded as not directly required: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from matplotlib>=2.0.0->scikit-image) (1.0.1)\n",
      "Requirement not upgraded as not directly required: setuptools in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->scikit-image) (41.0.1)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'show-attend-and-tell-tensorflow'...\n",
      "remote: Enumerating objects: 562, done.\u001b[K\n",
      "remote: Total 562 (delta 0), reused 0 (delta 0), pack-reused 562\u001b[K\n",
      "Receiving objects: 100% (562/562), 50.23 MiB | 12.51 MiB/s, done.\n",
      "Resolving deltas: 100% (301/301), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/yunjey/show-attend-and-tell-tensorflow.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'coco-caption'...\n",
      "remote: Enumerating objects: 736, done.\u001b[K\n",
      "remote: Total 736 (delta 0), reused 0 (delta 0), pack-reused 736\u001b[K\n",
      "Receiving objects: 100% (736/736), 130.04 MiB | 15.85 MiB/s, done.\n",
      "Resolving deltas: 100% (390/390), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tylin/coco-caption.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'show-attend-and-tell-tensorflow'\n",
      "/home/ec2-user/SageMaker/show-attend-and-tell-tensorflow\n"
     ]
    }
   ],
   "source": [
    "cd show-attend-and-tell-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from -r requirements.txt (line 1)) (1.16.4)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from -r requirements.txt (line 3)) (1.2.1)\n",
      "Requirement already satisfied: scikit-image in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from -r requirements.txt (line 4)) (0.14.5)\n",
      "Requirement already satisfied: hickle in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from -r requirements.txt (line 5)) (3.4.5)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from -r requirements.txt (line 6)) (5.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from matplotlib->-r requirements.txt (line 2)) (2.7.3)\n",
      "Requirement already satisfied: subprocess32 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from matplotlib->-r requirements.txt (line 2)) (3.5.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from matplotlib->-r requirements.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: six>=1.10 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.11.0)\n",
      "Requirement already satisfied: backports.functools-lru-cache in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.5)\n",
      "Requirement already satisfied: pytz in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from matplotlib->-r requirements.txt (line 2)) (2018.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from matplotlib->-r requirements.txt (line 2)) (2.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: networkx>=1.8 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from scikit-image->-r requirements.txt (line 4)) (2.1)\n",
      "Requirement already satisfied: cloudpickle>=0.2.1 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from scikit-image->-r requirements.txt (line 4)) (0.5.3)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from scikit-image->-r requirements.txt (line 4)) (0.5.2)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from hickle->-r requirements.txt (line 5)) (0.3.1.1)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from hickle->-r requirements.txt (line 5)) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from kiwisolver>=1.0.1->matplotlib->-r requirements.txt (line 2)) (41.0.1)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages (from networkx>=1.8->scikit-image->-r requirements.txt (line 4)) (4.3.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./download.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-06 11:57:35--  http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip\n",
      "Resolving msvocds.blob.core.windows.net (msvocds.blob.core.windows.net)... 52.176.224.96\n",
      "Connecting to msvocds.blob.core.windows.net (msvocds.blob.core.windows.net)|52.176.224.96|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19673183 (19M) [application/octet-stream Charset=UTF-8]\n",
      "Saving to: ‘data/captions_train-val2014.zip.1’\n",
      "\n",
      "captions_train-val2 100%[===================>]  18.76M  4.44MB/s    in 4.4s    \n",
      "\n",
      "2019-11-06 11:57:40 (4.23 MB/s) - ‘data/captions_train-val2014.zip.1’ saved [19673183/19673183]\n",
      "\n",
      "--2019-11-06 11:57:40--  http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat\n",
      "Resolving www.vlfeat.org (www.vlfeat.org)... 64.90.48.57\n",
      "Connecting to www.vlfeat.org (www.vlfeat.org)|64.90.48.57|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 534904783 (510M)\n",
      "Saving to: ‘data/imagenet-vgg-verydeep-19.mat.1’\n",
      "\n",
      "imagenet-vgg-veryde 100%[===================>] 510.12M  8.30MB/s    in 62s     \n",
      "\n",
      "2019-11-06 11:58:43 (8.23 MB/s) - ‘data/imagenet-vgg-verydeep-19.mat.1’ saved [534904783/534904783]\n",
      "\n",
      "--2019-11-06 11:58:43--  http://msvocds.blob.core.windows.net/coco2014/train2014.zip\n",
      "Resolving msvocds.blob.core.windows.net (msvocds.blob.core.windows.net)... 52.176.224.96\n",
      "Connecting to msvocds.blob.core.windows.net (msvocds.blob.core.windows.net)|52.176.224.96|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13510573713 (13G) [application/octet-stream]\n",
      "Saving to: ‘image/train2014.zip’\n",
      "\n",
      "train2014.zip        48%[========>           ]   6.12G  --.-KB/s    in 32m 58s \n",
      "\n",
      "2019-11-06 12:31:42 (3.17 MB/s) - Read error at byte 6572474368/13510573713 (Connection timed out). Retrying.\n",
      "\n",
      "--2019-11-06 12:31:43--  (try: 2)  http://msvocds.blob.core.windows.net/coco2014/train2014.zip\n",
      "Connecting to msvocds.blob.core.windows.net (msvocds.blob.core.windows.net)|52.176.224.96|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13510573713 (13G) [application/octet-stream]\n",
      "Saving to: ‘image/train2014.zip’\n",
      "\n",
      "train2014.zip         5%[>                   ] 661.53M  5.54MB/s    eta 48m 52s"
     ]
    }
   ],
   "source": [
    "!./download.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python resize.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepro.py:40: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  caption_data.set_value(i, 'caption', caption.lower())\n",
      "The number of captions before deletion: 414113\n",
      "The number of captions after deletion: 399998\n",
      "The number of captions before deletion: 202654\n",
      "The number of captions after deletion: 195891\n",
      "Finished processing caption data\n",
      "Saved data/train/train.annotations.pkl..\n",
      "Saved data/val/val.annotations.pkl..\n",
      "Saved data/test/test.annotations.pkl..\n",
      "Loaded ./data/train/train.annotations.pkl..\n",
      "Filtered 23107 words to 23107 words with word count threshold 1.\n",
      "Max length of caption:  15\n",
      "Saved ./data/train/word_to_idx.pkl..\n",
      "Finished building caption vectors\n",
      "Saved ./data/train/train.captions.pkl..\n",
      "Saved ./data/train/train.file.names.pkl..\n",
      "Saved ./data/train/train.image.idxs.pkl..\n",
      "Saved ./data/train/train.references.pkl..\n",
      "Finished building train caption dataset\n",
      "Loaded ./data/val/val.annotations.pkl..\n",
      "Finished building caption vectors\n",
      "Saved ./data/val/val.captions.pkl..\n",
      "Saved ./data/val/val.file.names.pkl..\n",
      "Saved ./data/val/val.image.idxs.pkl..\n",
      "Saved ./data/val/val.references.pkl..\n",
      "Finished building val caption dataset\n",
      "Loaded ./data/test/test.annotations.pkl..\n",
      "Finished building caption vectors\n",
      "Saved ./data/test/test.captions.pkl..\n",
      "Saved ./data/test/test.file.names.pkl..\n",
      "Saved ./data/test/test.image.idxs.pkl..\n",
      "Saved ./data/test/test.references.pkl..\n",
      "Finished building test caption dataset\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1106 21:22:17.585788 139630576125760 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/show-attend-and-tell-tensorflow/core/vggnet.py:16: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1106 21:22:21.813214 139630576125760 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/show-attend-and-tell-tensorflow/core/vggnet.py:22: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W1106 21:22:21.813767 139630576125760 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/show-attend-and-tell-tensorflow/core/vggnet.py:30: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W1106 21:22:32.623800 139630576125760 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/show-attend-and-tell-tensorflow/core/vggnet.py:40: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1106 21:22:32.655494 139630576125760 deprecation_wrapper.py:119] From prepro.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2019-11-06 21:22:32.686397: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300050000 Hz\n",
      "2019-11-06 21:22:32.686529: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555b64dd8020 executing computations on platform Host. Devices:\n",
      "2019-11-06 21:22:32.686552: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-11-06 21:22:32.686650: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "W1106 21:22:32.686851 139630576125760 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "2019-11-06 21:22:34.358155: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "Loaded ./data/train/train.annotations.pkl..\n",
      "Processed 100 train features..\n",
      "Processed 200 train features..\n",
      "Processed 300 train features..\n",
      "Processed 400 train features..\n",
      "Processed 500 train features..\n",
      "Processed 600 train features..\n",
      "Processed 700 train features..\n",
      "Processed 800 train features..\n",
      "Processed 900 train features..\n",
      "Processed 1000 train features..\n",
      "Processed 1100 train features..\n",
      "Processed 1200 train features..\n",
      "Processed 1300 train features..\n",
      "Processed 1400 train features..\n",
      "Processed 1500 train features..\n",
      "Processed 1600 train features..\n",
      "Processed 1700 train features..\n",
      "Processed 1800 train features..\n",
      "Processed 1900 train features..\n",
      "Processed 2000 train features..\n",
      "Processed 2100 train features..\n",
      "Processed 2200 train features..\n",
      "Processed 2300 train features..\n",
      "Processed 2400 train features..\n",
      "Processed 2500 train features..\n",
      "Processed 2600 train features..\n",
      "Processed 2700 train features..\n",
      "Processed 2800 train features..\n",
      "Processed 2900 train features..\n",
      "Processed 3000 train features..\n",
      "Processed 3100 train features..\n",
      "Processed 3200 train features..\n",
      "Processed 3300 train features..\n",
      "Processed 3400 train features..\n",
      "Processed 3500 train features..\n",
      "Processed 3600 train features..\n",
      "Processed 3700 train features..\n",
      "Processed 3800 train features..\n",
      "Processed 3900 train features..\n",
      "Processed 4000 train features..\n",
      "Processed 4100 train features..\n",
      "Processed 4200 train features..\n",
      "Processed 4300 train features..\n",
      "Processed 4400 train features..\n",
      "Processed 4500 train features..\n",
      "Processed 4600 train features..\n",
      "Processed 4700 train features..\n",
      "Processed 4800 train features..\n",
      "Processed 4900 train features..\n",
      "Processed 5000 train features..\n",
      "Processed 5100 train features..\n",
      "Processed 5200 train features..\n",
      "Processed 5300 train features..\n",
      "Processed 5400 train features..\n",
      "Processed 5500 train features..\n",
      "Processed 5600 train features..\n",
      "Processed 5700 train features..\n",
      "Processed 5800 train features..\n",
      "Processed 5900 train features..\n",
      "Processed 6000 train features..\n",
      "Processed 6100 train features..\n",
      "Processed 6200 train features..\n",
      "Processed 6300 train features..\n",
      "Processed 6400 train features..\n",
      "Processed 6500 train features..\n",
      "Processed 6600 train features..\n",
      "Processed 6700 train features..\n",
      "Processed 6800 train features..\n",
      "Processed 6900 train features..\n",
      "Processed 7000 train features..\n",
      "Processed 7100 train features..\n",
      "Processed 7200 train features..\n",
      "Processed 7300 train features..\n",
      "Processed 7400 train features..\n",
      "Processed 7500 train features..\n",
      "Processed 7600 train features..\n",
      "Processed 7700 train features..\n",
      "Processed 7800 train features..\n",
      "Processed 7900 train features..\n",
      "Processed 8000 train features..\n",
      "Processed 8100 train features..\n",
      "Processed 8200 train features..\n",
      "Processed 8300 train features..\n",
      "Processed 8400 train features..\n",
      "Processed 8500 train features..\n",
      "Processed 8600 train features..\n",
      "Processed 8700 train features..\n",
      "Processed 8800 train features..\n",
      "Processed 8900 train features..\n",
      "Processed 9000 train features..\n",
      "Processed 9100 train features..\n",
      "Processed 9200 train features..\n",
      "Processed 9300 train features..\n",
      "Processed 9400 train features..\n",
      "Processed 9500 train features..\n",
      "Processed 9600 train features..\n",
      "Processed 9700 train features..\n",
      "Processed 9800 train features..\n",
      "Processed 9900 train features..\n",
      "Processed 10000 train features..\n",
      "Processed 10100 train features..\n",
      "Processed 10200 train features..\n",
      "Processed 10300 train features..\n",
      "Processed 10400 train features..\n",
      "Processed 10500 train features..\n",
      "Processed 10600 train features..\n",
      "Processed 10700 train features..\n",
      "Processed 10800 train features..\n",
      "Processed 10900 train features..\n",
      "Processed 11000 train features..\n",
      "Processed 11100 train features..\n",
      "Processed 11200 train features..\n",
      "Processed 11300 train features..\n",
      "Processed 11400 train features..\n",
      "Processed 11500 train features..\n",
      "Processed 11600 train features..\n",
      "Processed 11700 train features..\n",
      "Processed 11800 train features..\n",
      "Processed 11900 train features..\n",
      "Processed 12000 train features..\n",
      "Processed 12100 train features..\n",
      "Processed 12200 train features..\n",
      "Processed 12300 train features..\n",
      "Processed 12400 train features..\n",
      "Processed 12500 train features..\n",
      "Processed 12600 train features..\n",
      "Processed 12700 train features..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 12800 train features..\n",
      "Processed 12900 train features..\n",
      "Processed 13000 train features..\n",
      "Processed 13100 train features..\n",
      "Processed 13200 train features..\n",
      "Processed 13300 train features..\n",
      "Processed 13400 train features..\n",
      "Processed 13500 train features..\n",
      "Processed 13600 train features..\n",
      "Processed 13700 train features..\n",
      "Processed 13800 train features..\n",
      "Processed 13900 train features..\n",
      "Processed 14000 train features..\n",
      "Processed 14100 train features..\n",
      "Processed 14200 train features..\n",
      "Processed 14300 train features..\n",
      "Processed 14400 train features..\n",
      "Processed 14500 train features..\n",
      "Processed 14600 train features..\n",
      "Processed 14700 train features..\n",
      "Processed 14800 train features..\n",
      "Processed 14900 train features..\n",
      "Processed 15000 train features..\n",
      "Processed 15100 train features..\n",
      "Processed 15200 train features..\n",
      "Processed 15300 train features..\n",
      "Processed 15400 train features..\n",
      "Processed 15500 train features..\n",
      "Processed 15600 train features..\n",
      "Processed 15700 train features..\n",
      "Processed 15800 train features..\n",
      "Processed 15900 train features..\n",
      "Processed 16000 train features..\n",
      "Processed 16100 train features..\n",
      "Processed 16200 train features..\n",
      "Processed 16300 train features..\n",
      "Processed 16400 train features..\n",
      "Processed 16500 train features..\n",
      "Processed 16600 train features..\n",
      "Processed 16700 train features..\n",
      "Processed 16800 train features..\n",
      "Processed 16900 train features..\n",
      "Processed 17000 train features..\n",
      "Processed 17100 train features..\n",
      "Processed 17200 train features..\n",
      "Processed 17300 train features..\n",
      "Processed 17400 train features..\n",
      "Processed 17500 train features..\n",
      "Processed 17600 train features..\n",
      "Processed 17700 train features..\n",
      "Processed 17800 train features..\n",
      "Processed 17900 train features..\n",
      "Processed 18000 train features..\n",
      "Processed 18100 train features..\n",
      "Processed 18200 train features..\n",
      "Processed 18300 train features..\n",
      "Processed 18400 train features..\n",
      "Processed 18500 train features..\n",
      "Processed 18600 train features..\n",
      "Processed 18700 train features..\n",
      "Processed 18800 train features..\n",
      "Processed 18900 train features..\n",
      "Processed 19000 train features..\n",
      "Processed 19100 train features..\n",
      "Processed 19200 train features..\n",
      "Processed 19300 train features..\n",
      "Processed 19400 train features..\n",
      "Processed 19500 train features..\n",
      "Processed 19600 train features..\n",
      "Processed 19700 train features..\n",
      "Processed 19800 train features..\n",
      "Processed 19900 train features..\n",
      "Processed 20000 train features..\n",
      "Processed 20100 train features..\n",
      "Processed 20200 train features..\n",
      "Processed 20300 train features..\n",
      "Processed 20400 train features..\n",
      "Processed 20500 train features..\n",
      "Processed 20600 train features..\n",
      "Processed 20700 train features..\n",
      "Processed 20800 train features..\n",
      "Processed 20900 train features..\n",
      "Processed 21000 train features..\n",
      "Processed 21100 train features..\n",
      "Processed 21200 train features..\n",
      "Processed 21300 train features..\n",
      "Processed 21400 train features..\n",
      "Processed 21500 train features..\n",
      "Processed 21600 train features..\n",
      "Processed 21700 train features..\n",
      "Processed 21800 train features..\n",
      "Processed 21900 train features..\n",
      "Processed 22000 train features..\n",
      "Processed 22100 train features..\n",
      "Processed 22200 train features..\n",
      "Processed 22300 train features..\n",
      "Processed 22400 train features..\n",
      "Processed 22500 train features..\n",
      "Processed 22600 train features..\n",
      "Processed 22700 train features..\n",
      "Processed 22800 train features..\n",
      "Processed 22900 train features..\n",
      "Processed 23000 train features..\n",
      "Processed 23100 train features..\n",
      "Processed 23200 train features..\n",
      "Processed 23300 train features..\n",
      "Processed 23400 train features..\n",
      "Processed 23500 train features..\n",
      "Processed 23600 train features..\n",
      "Processed 23700 train features..\n",
      "Processed 23800 train features..\n",
      "Processed 23900 train features..\n",
      "Processed 24000 train features..\n",
      "Processed 24100 train features..\n",
      "Processed 24200 train features..\n",
      "Processed 24300 train features..\n",
      "Processed 24400 train features..\n",
      "Processed 24500 train features..\n",
      "Processed 24600 train features..\n",
      "Processed 24700 train features..\n",
      "Processed 24800 train features..\n",
      "Processed 24900 train features..\n",
      "Processed 25000 train features..\n",
      "Processed 25100 train features..\n",
      "Processed 25200 train features..\n",
      "Processed 25300 train features..\n",
      "Processed 25400 train features..\n",
      "Processed 25500 train features..\n",
      "Processed 25600 train features..\n",
      "Processed 25700 train features..\n",
      "Processed 25800 train features..\n",
      "Processed 25900 train features..\n",
      "Processed 26000 train features..\n",
      "Processed 26100 train features..\n",
      "Processed 26200 train features..\n",
      "Processed 26300 train features..\n",
      "Processed 26400 train features..\n",
      "Processed 26500 train features..\n",
      "Processed 26600 train features..\n",
      "Processed 26700 train features..\n",
      "Processed 26800 train features..\n",
      "Processed 26900 train features..\n",
      "Processed 27000 train features..\n",
      "Processed 27100 train features..\n",
      "Processed 27200 train features..\n",
      "Processed 27300 train features..\n",
      "Processed 27400 train features..\n",
      "Processed 27500 train features..\n",
      "Processed 27600 train features..\n",
      "Processed 27700 train features..\n",
      "Processed 27800 train features..\n",
      "Processed 27900 train features..\n",
      "Processed 28000 train features..\n",
      "Processed 28100 train features..\n",
      "Processed 28200 train features..\n",
      "Processed 28300 train features..\n",
      "Processed 28400 train features..\n",
      "Processed 28500 train features..\n",
      "Processed 28600 train features..\n",
      "Processed 28700 train features..\n",
      "Processed 28800 train features..\n",
      "Processed 28900 train features..\n",
      "Processed 29000 train features..\n",
      "Processed 29100 train features..\n",
      "Processed 29200 train features..\n",
      "Processed 29300 train features..\n",
      "Processed 29400 train features..\n",
      "Processed 29500 train features..\n",
      "Processed 29600 train features..\n",
      "Processed 29700 train features..\n",
      "Processed 29800 train features..\n",
      "Processed 29900 train features..\n",
      "Processed 30000 train features..\n",
      "Processed 30100 train features..\n",
      "Processed 30200 train features..\n",
      "Processed 30300 train features..\n",
      "Processed 30400 train features..\n",
      "Processed 30500 train features..\n",
      "Processed 30600 train features..\n",
      "Processed 30700 train features..\n",
      "Processed 30800 train features..\n",
      "Processed 30900 train features..\n",
      "Processed 31000 train features..\n",
      "Processed 31100 train features..\n",
      "Processed 31200 train features..\n",
      "Processed 31300 train features..\n",
      "Processed 31400 train features..\n",
      "Processed 31500 train features..\n",
      "Processed 31600 train features..\n",
      "Processed 31700 train features..\n",
      "Processed 31800 train features..\n",
      "Processed 31900 train features..\n",
      "Processed 32000 train features..\n",
      "Processed 32100 train features..\n",
      "Processed 32200 train features..\n",
      "Processed 32300 train features..\n",
      "Processed 32400 train features..\n",
      "Processed 32500 train features..\n",
      "Processed 32600 train features..\n",
      "Processed 32700 train features..\n",
      "Processed 32800 train features..\n",
      "Processed 32900 train features..\n",
      "Processed 33000 train features..\n",
      "Processed 33100 train features..\n",
      "Processed 33200 train features..\n",
      "Processed 33300 train features..\n",
      "Processed 33400 train features..\n",
      "Processed 33500 train features..\n",
      "Processed 33600 train features..\n",
      "Processed 33700 train features..\n",
      "Processed 33800 train features..\n",
      "Processed 33900 train features..\n",
      "Processed 34000 train features..\n",
      "Processed 34100 train features..\n",
      "Processed 34200 train features..\n",
      "Processed 34300 train features..\n",
      "Processed 34400 train features..\n",
      "Processed 34500 train features..\n",
      "Processed 34600 train features..\n",
      "Processed 34700 train features..\n",
      "Processed 34800 train features..\n",
      "Processed 34900 train features..\n",
      "Processed 35000 train features..\n",
      "Processed 35100 train features..\n",
      "Processed 35200 train features..\n",
      "Processed 35300 train features..\n",
      "Processed 35400 train features..\n",
      "Processed 35500 train features..\n",
      "Processed 35600 train features..\n",
      "Processed 35700 train features..\n",
      "Processed 35800 train features..\n",
      "Processed 35900 train features..\n",
      "Processed 36000 train features..\n",
      "Processed 36100 train features..\n",
      "Processed 36200 train features..\n",
      "Processed 36300 train features..\n",
      "Processed 36400 train features..\n",
      "Processed 36500 train features..\n",
      "Processed 36600 train features..\n",
      "Processed 36700 train features..\n",
      "Processed 36800 train features..\n",
      "Processed 36900 train features..\n",
      "Processed 37000 train features..\n",
      "Processed 37100 train features..\n",
      "Processed 37200 train features..\n",
      "Processed 37300 train features..\n",
      "Processed 37400 train features..\n",
      "Processed 37500 train features..\n",
      "Processed 37600 train features..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 37700 train features..\n",
      "Processed 37800 train features..\n",
      "Processed 37900 train features..\n",
      "Processed 38000 train features..\n",
      "Processed 38100 train features..\n",
      "Processed 38200 train features..\n",
      "Processed 38300 train features..\n",
      "Processed 38400 train features..\n",
      "Processed 38500 train features..\n",
      "Processed 38600 train features..\n",
      "Processed 38700 train features..\n",
      "Processed 38800 train features..\n",
      "Processed 38900 train features..\n",
      "Processed 39000 train features..\n",
      "Processed 39100 train features..\n",
      "Processed 39200 train features..\n",
      "Processed 39300 train features..\n",
      "Processed 39400 train features..\n",
      "Processed 39500 train features..\n",
      "Processed 39600 train features..\n",
      "Processed 39700 train features..\n",
      "Processed 39800 train features..\n",
      "Processed 39900 train features..\n",
      "Processed 40000 train features..\n",
      "Processed 40100 train features..\n",
      "Processed 40200 train features..\n",
      "Processed 40300 train features..\n",
      "Processed 40400 train features..\n",
      "Processed 40500 train features..\n",
      "Processed 40600 train features..\n",
      "Processed 40700 train features..\n",
      "Processed 40800 train features..\n",
      "Processed 40900 train features..\n",
      "Processed 41000 train features..\n",
      "Processed 41100 train features..\n",
      "Processed 41200 train features..\n",
      "Processed 41300 train features..\n",
      "Processed 41400 train features..\n",
      "Processed 41500 train features..\n",
      "Processed 41600 train features..\n",
      "Processed 41700 train features..\n",
      "Processed 41800 train features..\n",
      "Processed 41900 train features..\n",
      "Processed 42000 train features..\n",
      "Processed 42100 train features..\n",
      "Processed 42200 train features..\n",
      "Processed 42300 train features..\n",
      "Processed 42400 train features..\n",
      "Processed 42500 train features..\n",
      "Processed 42600 train features..\n",
      "Processed 42700 train features..\n",
      "Processed 42800 train features..\n",
      "Processed 42900 train features..\n",
      "Processed 43000 train features..\n",
      "Processed 43100 train features..\n",
      "Processed 43200 train features..\n",
      "Processed 43300 train features..\n",
      "Processed 43400 train features..\n",
      "Processed 43500 train features..\n",
      "Processed 43600 train features..\n",
      "Processed 43700 train features..\n",
      "Processed 43800 train features..\n",
      "Processed 43900 train features..\n",
      "Processed 44000 train features..\n",
      "Processed 44100 train features..\n",
      "Processed 44200 train features..\n",
      "Processed 44300 train features..\n",
      "Processed 44400 train features..\n",
      "Processed 44500 train features..\n",
      "Processed 44600 train features..\n",
      "Processed 44700 train features..\n",
      "Processed 44800 train features..\n",
      "Processed 44900 train features..\n",
      "Processed 45000 train features..\n",
      "Processed 45100 train features..\n",
      "Processed 45200 train features..\n",
      "Processed 45300 train features..\n",
      "Processed 45400 train features..\n",
      "Processed 45500 train features..\n",
      "Processed 45600 train features..\n",
      "Processed 45700 train features..\n",
      "Processed 45800 train features..\n",
      "Processed 45900 train features..\n",
      "Processed 46000 train features..\n",
      "Processed 46100 train features..\n",
      "Processed 46200 train features..\n",
      "Processed 46300 train features..\n",
      "Processed 46400 train features..\n",
      "Processed 46500 train features..\n",
      "Processed 46600 train features..\n",
      "Processed 46700 train features..\n",
      "Processed 46800 train features..\n",
      "Processed 46900 train features..\n",
      "Processed 47000 train features..\n",
      "Processed 47100 train features..\n",
      "Processed 47200 train features..\n",
      "Processed 47300 train features..\n",
      "Processed 47400 train features..\n",
      "Processed 47500 train features..\n",
      "Processed 47600 train features..\n",
      "Processed 47700 train features..\n",
      "Processed 47800 train features..\n",
      "Processed 47900 train features..\n",
      "Processed 48000 train features..\n",
      "Processed 48100 train features..\n",
      "Processed 48200 train features..\n",
      "Processed 48300 train features..\n",
      "Processed 48400 train features..\n",
      "Processed 48500 train features..\n",
      "Processed 48600 train features..\n",
      "Processed 48700 train features..\n",
      "Processed 48800 train features..\n",
      "Processed 48900 train features..\n",
      "Processed 49000 train features..\n",
      "Processed 49100 train features..\n",
      "Processed 49200 train features..\n",
      "Processed 49300 train features..\n",
      "Processed 49400 train features..\n",
      "Processed 49500 train features..\n",
      "Processed 49600 train features..\n",
      "Processed 49700 train features..\n",
      "Processed 49800 train features..\n",
      "Processed 49900 train features..\n",
      "Processed 50000 train features..\n",
      "Processed 50100 train features..\n",
      "Processed 50200 train features..\n",
      "Processed 50300 train features..\n",
      "Processed 50400 train features..\n",
      "Processed 50500 train features..\n",
      "Processed 50600 train features..\n",
      "Processed 50700 train features..\n",
      "Processed 50800 train features..\n",
      "Processed 50900 train features..\n",
      "Processed 51000 train features..\n",
      "Processed 51100 train features..\n",
      "Processed 51200 train features..\n",
      "Processed 51300 train features..\n",
      "Processed 51400 train features..\n",
      "Processed 51500 train features..\n",
      "Processed 51600 train features..\n",
      "Processed 51700 train features..\n",
      "Processed 51800 train features..\n",
      "Processed 51900 train features..\n",
      "Processed 52000 train features..\n",
      "Processed 52100 train features..\n",
      "Processed 52200 train features..\n",
      "Processed 52300 train features..\n",
      "Processed 52400 train features..\n",
      "Processed 52500 train features..\n",
      "Processed 52600 train features..\n",
      "Processed 52700 train features..\n",
      "Processed 52800 train features..\n",
      "Processed 52900 train features..\n",
      "Processed 53000 train features..\n",
      "Processed 53100 train features..\n",
      "Processed 53200 train features..\n",
      "Processed 53300 train features..\n",
      "Processed 53400 train features..\n",
      "Processed 53500 train features..\n",
      "Processed 53600 train features..\n",
      "Processed 53700 train features..\n",
      "Processed 53800 train features..\n",
      "Processed 53900 train features..\n",
      "Processed 54000 train features..\n",
      "Processed 54100 train features..\n",
      "Processed 54200 train features..\n",
      "Processed 54300 train features..\n",
      "Processed 54400 train features..\n",
      "Processed 54500 train features..\n",
      "Processed 54600 train features..\n",
      "Processed 54700 train features..\n",
      "Processed 54800 train features..\n",
      "Processed 54900 train features..\n",
      "Processed 55000 train features..\n",
      "Processed 55100 train features..\n",
      "Processed 55200 train features..\n",
      "Processed 55300 train features..\n",
      "Processed 55400 train features..\n",
      "Processed 55500 train features..\n",
      "Processed 55600 train features..\n",
      "Processed 55700 train features..\n",
      "Processed 55800 train features..\n",
      "Processed 55900 train features..\n",
      "Processed 56000 train features..\n",
      "Processed 56100 train features..\n",
      "Processed 56200 train features..\n",
      "Processed 56300 train features..\n",
      "Processed 56400 train features..\n",
      "Processed 56500 train features..\n",
      "Processed 56600 train features..\n",
      "Processed 56700 train features..\n",
      "Processed 56800 train features..\n",
      "Processed 56900 train features..\n",
      "Processed 57000 train features..\n",
      "Processed 57100 train features..\n",
      "Processed 57200 train features..\n",
      "Processed 57300 train features..\n",
      "Processed 57400 train features..\n",
      "Processed 57500 train features..\n",
      "Processed 57600 train features..\n",
      "Processed 57700 train features..\n",
      "Processed 57800 train features..\n",
      "Processed 57900 train features..\n",
      "Processed 58000 train features..\n",
      "Processed 58100 train features..\n",
      "Processed 58200 train features..\n",
      "Processed 58300 train features..\n",
      "Processed 58400 train features..\n",
      "Processed 58500 train features..\n",
      "Processed 58600 train features..\n",
      "Processed 58700 train features..\n",
      "Processed 58800 train features..\n",
      "Processed 58900 train features..\n",
      "Processed 59000 train features..\n",
      "Processed 59100 train features..\n",
      "Processed 59200 train features..\n",
      "Processed 59300 train features..\n",
      "Processed 59400 train features..\n",
      "Processed 59500 train features..\n",
      "Processed 59600 train features..\n",
      "Processed 59700 train features..\n",
      "Processed 59800 train features..\n",
      "Processed 59900 train features..\n",
      "Processed 60000 train features..\n",
      "Processed 60100 train features..\n",
      "Processed 60200 train features..\n",
      "Processed 60300 train features..\n",
      "Processed 60400 train features..\n",
      "Processed 60500 train features..\n",
      "Processed 60600 train features..\n",
      "Processed 60700 train features..\n",
      "Processed 60800 train features..\n",
      "Processed 60900 train features..\n",
      "Processed 61000 train features..\n",
      "Processed 61100 train features..\n",
      "Processed 61200 train features..\n",
      "Processed 61300 train features..\n",
      "Processed 61400 train features..\n",
      "Processed 61500 train features..\n",
      "Processed 61600 train features..\n",
      "Processed 61700 train features..\n",
      "Processed 61800 train features..\n",
      "Processed 61900 train features..\n",
      "Processed 62000 train features..\n",
      "Processed 62100 train features..\n",
      "Processed 62200 train features..\n",
      "Processed 62300 train features..\n",
      "Processed 62400 train features..\n",
      "Processed 62500 train features..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 62600 train features..\n",
      "Processed 62700 train features..\n",
      "Processed 62800 train features..\n",
      "Processed 62900 train features..\n",
      "Processed 63000 train features..\n",
      "Processed 63100 train features..\n",
      "Processed 63200 train features..\n",
      "Processed 63300 train features..\n",
      "Processed 63400 train features..\n",
      "Processed 63500 train features..\n",
      "Processed 63600 train features..\n",
      "Processed 63700 train features..\n",
      "Processed 63800 train features..\n",
      "Processed 63900 train features..\n",
      "Processed 64000 train features..\n",
      "Processed 64100 train features..\n",
      "Processed 64200 train features..\n",
      "Processed 64300 train features..\n",
      "Processed 64400 train features..\n",
      "Processed 64500 train features..\n",
      "Processed 64600 train features..\n",
      "Processed 64700 train features..\n",
      "Processed 64800 train features..\n",
      "Processed 64900 train features..\n",
      "Processed 65000 train features..\n",
      "Processed 65100 train features..\n",
      "Processed 65200 train features..\n",
      "Processed 65300 train features..\n",
      "Processed 65400 train features..\n",
      "Processed 65500 train features..\n",
      "Processed 65600 train features..\n",
      "Processed 65700 train features..\n",
      "Processed 65800 train features..\n",
      "Processed 65900 train features..\n",
      "Processed 66000 train features..\n",
      "Processed 66100 train features..\n",
      "Processed 66200 train features..\n",
      "Processed 66300 train features..\n",
      "Processed 66400 train features..\n",
      "Processed 66500 train features..\n",
      "Processed 66600 train features..\n",
      "Processed 66700 train features..\n",
      "Processed 66800 train features..\n",
      "Processed 66900 train features..\n",
      "Processed 67000 train features..\n",
      "Processed 67100 train features..\n",
      "Processed 67200 train features..\n",
      "Processed 67300 train features..\n",
      "Processed 67400 train features..\n",
      "Processed 67500 train features..\n",
      "Processed 67600 train features..\n",
      "Processed 67700 train features..\n",
      "Processed 67800 train features..\n",
      "Processed 67900 train features..\n",
      "Processed 68000 train features..\n",
      "Processed 68100 train features..\n",
      "Processed 68200 train features..\n",
      "Processed 68300 train features..\n",
      "Processed 68400 train features..\n",
      "Processed 68500 train features..\n",
      "Processed 68600 train features..\n",
      "Processed 68700 train features..\n",
      "Processed 68800 train features..\n",
      "Processed 68900 train features..\n",
      "Processed 69000 train features..\n",
      "Processed 69100 train features..\n",
      "Processed 69200 train features..\n",
      "Processed 69300 train features..\n",
      "Processed 69400 train features..\n",
      "Processed 69500 train features..\n",
      "Processed 69600 train features..\n",
      "Processed 69700 train features..\n",
      "Processed 69800 train features..\n",
      "Processed 69900 train features..\n",
      "Processed 70000 train features..\n",
      "Processed 70100 train features..\n",
      "Processed 70200 train features..\n",
      "Processed 70300 train features..\n",
      "Processed 70400 train features..\n",
      "Processed 70500 train features..\n",
      "Processed 70600 train features..\n",
      "Processed 70700 train features..\n",
      "Processed 70800 train features..\n",
      "Processed 70900 train features..\n",
      "Processed 71000 train features..\n",
      "Processed 71100 train features..\n",
      "Processed 71200 train features..\n",
      "Processed 71300 train features..\n",
      "Processed 71400 train features..\n",
      "Processed 71500 train features..\n",
      "Processed 71600 train features..\n",
      "Processed 71700 train features..\n",
      "Processed 71800 train features..\n",
      "Processed 71900 train features..\n",
      "Processed 72000 train features..\n",
      "Processed 72100 train features..\n",
      "Processed 72200 train features..\n",
      "Processed 72300 train features..\n",
      "Processed 72400 train features..\n",
      "Processed 72500 train features..\n",
      "Processed 72600 train features..\n",
      "Processed 72700 train features..\n",
      "Processed 72800 train features..\n",
      "Processed 72900 train features..\n",
      "Processed 73000 train features..\n",
      "Processed 73100 train features..\n",
      "Processed 73200 train features..\n",
      "Processed 73300 train features..\n",
      "Processed 73400 train features..\n",
      "Processed 73500 train features..\n",
      "Processed 73600 train features..\n",
      "Processed 73700 train features..\n",
      "Processed 73800 train features..\n",
      "Processed 73900 train features..\n",
      "Processed 74000 train features..\n",
      "Processed 74100 train features..\n",
      "Processed 74200 train features..\n",
      "Processed 74300 train features..\n",
      "Processed 74400 train features..\n",
      "Processed 74500 train features..\n",
      "Processed 74600 train features..\n",
      "Processed 74700 train features..\n",
      "Processed 74800 train features..\n",
      "Processed 74900 train features..\n",
      "Processed 75000 train features..\n",
      "Processed 75100 train features..\n",
      "Processed 75200 train features..\n",
      "Processed 75300 train features..\n",
      "Processed 75400 train features..\n",
      "Processed 75500 train features..\n",
      "Processed 75600 train features..\n",
      "Processed 75700 train features..\n",
      "Processed 75800 train features..\n",
      "Processed 75900 train features..\n",
      "Processed 76000 train features..\n",
      "Processed 76100 train features..\n",
      "Processed 76200 train features..\n",
      "Processed 76300 train features..\n",
      "Processed 76400 train features..\n",
      "Processed 76500 train features..\n",
      "Processed 76600 train features..\n",
      "Processed 76700 train features..\n",
      "Processed 76800 train features..\n",
      "Processed 76900 train features..\n",
      "Processed 77000 train features..\n",
      "Processed 77100 train features..\n",
      "Processed 77200 train features..\n",
      "Processed 77300 train features..\n",
      "Processed 77400 train features..\n",
      "Processed 77500 train features..\n",
      "Processed 77600 train features..\n",
      "Processed 77700 train features..\n",
      "Processed 77800 train features..\n",
      "Processed 77900 train features..\n",
      "Processed 78000 train features..\n",
      "Processed 78100 train features..\n",
      "Processed 78200 train features..\n",
      "Processed 78300 train features..\n",
      "Processed 78400 train features..\n",
      "Processed 78500 train features..\n",
      "Processed 78600 train features..\n",
      "Processed 78700 train features..\n",
      "Processed 78800 train features..\n",
      "Processed 78900 train features..\n",
      "Processed 79000 train features..\n",
      "Processed 79100 train features..\n",
      "Processed 79200 train features..\n",
      "Processed 79300 train features..\n",
      "Processed 79400 train features..\n",
      "Processed 79500 train features..\n",
      "Processed 79600 train features..\n",
      "Processed 79700 train features..\n",
      "Processed 79800 train features..\n",
      "Processed 79900 train features..\n",
      "Processed 80000 train features..\n",
      "Processed 80100 train features..\n",
      "Processed 80200 train features..\n",
      "Processed 80300 train features..\n",
      "Processed 80400 train features..\n",
      "Processed 80500 train features..\n",
      "Processed 80600 train features..\n",
      "Processed 80700 train features..\n",
      "Processed 80800 train features..\n",
      "Processed 80900 train features..\n",
      "Processed 81000 train features..\n",
      "Processed 81100 train features..\n",
      "Processed 81200 train features..\n",
      "Processed 81300 train features..\n",
      "Processed 81400 train features..\n",
      "Processed 81500 train features..\n",
      "Processed 81600 train features..\n",
      "Processed 81700 train features..\n",
      "Processed 81800 train features..\n",
      "Processed 81900 train features..\n",
      "Processed 82000 train features..\n",
      "Processed 82100 train features..\n",
      "Processed 82200 train features..\n",
      "Processed 82300 train features..\n",
      "Processed 82400 train features..\n",
      "Processed 82500 train features..\n",
      "Processed 82600 train features..\n",
      "Processed 82700 train features..\n",
      "Processed 82800 train features..\n",
      "Saved ./data/train/train.features.hkl..\n",
      "Loaded ./data/val/val.annotations.pkl..\n",
      "Processed 100 val features..\n",
      "Processed 200 val features..\n",
      "Processed 300 val features..\n",
      "Processed 400 val features..\n",
      "Processed 500 val features..\n",
      "Processed 600 val features..\n",
      "Processed 700 val features..\n",
      "Processed 800 val features..\n",
      "Processed 900 val features..\n",
      "Processed 1000 val features..\n",
      "Processed 1100 val features..\n",
      "Processed 1200 val features..\n",
      "Processed 1300 val features..\n",
      "Processed 1400 val features..\n",
      "Processed 1500 val features..\n",
      "Processed 1600 val features..\n",
      "Processed 1700 val features..\n",
      "Processed 1800 val features..\n",
      "Processed 1900 val features..\n",
      "Processed 2000 val features..\n",
      "Processed 2100 val features..\n",
      "Processed 2200 val features..\n",
      "Processed 2300 val features..\n",
      "Processed 2400 val features..\n",
      "Processed 2500 val features..\n",
      "Processed 2600 val features..\n",
      "Processed 2700 val features..\n",
      "Processed 2800 val features..\n",
      "Processed 2900 val features..\n",
      "Processed 3000 val features..\n",
      "Processed 3100 val features..\n",
      "Processed 3200 val features..\n",
      "Processed 3300 val features..\n",
      "Processed 3400 val features..\n",
      "Processed 3500 val features..\n",
      "Processed 3600 val features..\n",
      "Processed 3700 val features..\n",
      "Processed 3800 val features..\n",
      "Processed 3900 val features..\n",
      "Processed 4000 val features..\n",
      "Processed 4100 val features..\n",
      "Saved ./data/val/val.features.hkl..\n",
      "Loaded ./data/test/test.annotations.pkl..\n",
      "Processed 100 test features..\n",
      "Processed 200 test features..\n",
      "Processed 300 test features..\n",
      "Processed 400 test features..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 test features..\n",
      "Processed 600 test features..\n",
      "Processed 700 test features..\n",
      "Processed 800 test features..\n",
      "Processed 900 test features..\n",
      "Processed 1000 test features..\n",
      "Processed 1100 test features..\n",
      "Processed 1200 test features..\n",
      "Processed 1300 test features..\n",
      "Processed 1400 test features..\n",
      "Processed 1500 test features..\n",
      "Processed 1600 test features..\n",
      "Processed 1700 test features..\n",
      "Processed 1800 test features..\n",
      "Processed 1900 test features..\n",
      "Processed 2000 test features..\n",
      "Processed 2100 test features..\n",
      "Processed 2200 test features..\n",
      "Processed 2300 test features..\n",
      "Processed 2400 test features..\n",
      "Processed 2500 test features..\n",
      "Processed 2600 test features..\n",
      "Processed 2700 test features..\n",
      "Processed 2800 test features..\n",
      "Processed 2900 test features..\n",
      "Processed 3000 test features..\n",
      "Processed 3100 test features..\n",
      "Processed 3200 test features..\n",
      "Processed 3300 test features..\n",
      "Processed 3400 test features..\n",
      "Processed 3500 test features..\n",
      "Processed 3600 test features..\n",
      "Processed 3700 test features..\n",
      "Processed 3800 test features..\n",
      "Processed 3900 test features..\n",
      "Processed 4000 test features..\n",
      "Processed 4100 test features..\n",
      "Saved ./data/test/test.features.hkl..\n"
     ]
    }
   ],
   "source": [
    "!python prepro.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_idxs <type 'numpy.ndarray'> (399998,) int32\n",
      "file_names <type 'numpy.ndarray'> (82783,) <U55\n",
      "word_to_idx <type 'dict'> 23110\n",
      "features <type 'numpy.ndarray'> (82783, 196, 512) float32\n",
      "captions <type 'numpy.ndarray'> (399998, 17) int32\n",
      "Elapse time: 17.99\n",
      "image_idxs <type 'numpy.ndarray'> (19589,) int32\n",
      "file_names <type 'numpy.ndarray'> (4052,) <U51\n",
      "features <type 'numpy.ndarray'> (4052, 196, 512) float32\n",
      "captions <type 'numpy.ndarray'> (19589, 17) int32\n",
      "Elapse time: 0.86\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1106 22:00:29.355180 140701423388480 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1106 22:00:29.355663 140701423388480 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/show-attend-and-tell-tensorflow/core/model.py:56: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1106 22:00:29.357621 140701423388480 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/show-attend-and-tell-tensorflow/core/solver.py:54: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W1106 22:00:29.357903 140701423388480 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/show-attend-and-tell-tensorflow/core/solver.py:80: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W1106 22:00:29.358042 140701423388480 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/show-attend-and-tell-tensorflow/core/solver.py:80: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "W1106 22:00:29.367053 140701423388480 deprecation.py:323] From /home/ec2-user/SageMaker/show-attend-and-tell-tensorflow/core/model.py:146: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1106 22:00:29.398699 140701423388480 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/show-attend-and-tell-tensorflow/core/model.py:63: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W1106 22:00:29.441066 140701423388480 deprecation.py:323] From /home/ec2-user/SageMaker/show-attend-and-tell-tensorflow/core/model.py:158: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W1106 22:00:29.487862 140701423388480 deprecation.py:506] From /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1106 22:00:29.495487 140701423388480 deprecation.py:506] From /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1106 22:00:29.925925 140701423388480 deprecation.py:506] From /home/ec2-user/SageMaker/show-attend-and-tell-tensorflow/core/model.py:114: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1106 22:00:31.786931 140701423388480 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/show-attend-and-tell-tensorflow/core/solver.py:88: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "W1106 22:00:31.847779 140701423388480 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/math_grad.py:1205: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1106 22:00:34.987859 140701423388480 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/show-attend-and-tell-tensorflow/core/solver.py:94: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W1106 22:00:34.989259 140701423388480 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/show-attend-and-tell-tensorflow/core/solver.py:97: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "W1106 22:00:35.033130 140701423388480 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/show-attend-and-tell-tensorflow/core/solver.py:103: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "The number of epoch: 20\n",
      "Data size: 82783\n",
      "Batch size: 128\n",
      "Iterations per epoch: 647\n",
      "2019-11-06 22:00:35.062239: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300050000 Hz\n",
      "2019-11-06 22:00:35.062368: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55baf822b270 executing computations on platform Host. Devices:\n",
      "2019-11-06 22:00:35.062385: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-11-06 22:00:35.062480: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2019-11-06 22:00:35.735471: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "W1106 22:00:36.541125 140701423388480 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/show-attend-and-tell-tensorflow/core/solver.py:116: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "W1106 22:00:37.480451 140701423388480 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/show-attend-and-tell-tensorflow/core/solver.py:117: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "Previous epoch loss:  -1\n",
      "Current epoch loss:  29214.90866470337\n",
      "Elapsed time:  1971.48182607\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 42158, 'guess': [42290, 38238, 34186, 30134], 'testlen': 42290, 'correct': [25978, 9282, 3289, 1201]}\n",
      "ratio: 1.00313107832\n",
      "Bleu_1:\t0.61428233625\n",
      "Bleu_2:\t0.386151042161\n",
      "Bleu_3:\t0.242983436093\n",
      "Bleu_4:\t0.154633668027\n",
      "METEOR:\t0.179033525517\n",
      "ROUGE_L: 0.4767142670449095\n",
      "CIDEr:\t0.432448751090482\n",
      "model-1 saved.\n",
      "Previous epoch loss:  29214.90866470337\n",
      "Current epoch loss:  22285.183755874634\n",
      "Elapsed time:  3985.755126\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 42368, 'guess': [42506, 38454, 34402, 30350], 'testlen': 42506, 'correct': [26362, 9996, 3823, 1522]}\n",
      "ratio: 1.00325717523\n",
      "Bleu_1:\t0.620194796029\n",
      "Bleu_2:\t0.401519296407\n",
      "Bleu_3:\t0.261664305587\n",
      "Bleu_4:\t0.17313000904\n",
      "METEOR:\t0.188409564939\n",
      "ROUGE_L: 0.48614659483311656\n",
      "CIDEr:\t0.5058974734236265\n",
      "model-2 saved.\n",
      "Previous epoch loss:  22285.183755874634\n",
      "Current epoch loss:  20537.19714164734\n",
      "Elapsed time:  6000.23488903\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 43387, 'guess': [43782, 39730, 35678, 31626], 'testlen': 43782, 'correct': [27493, 10694, 4152, 1720]}\n",
      "ratio: 1.00910410953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu_1:\t0.627952126445\n",
      "Bleu_2:\t0.411125178521\n",
      "Bleu_3:\t0.269940676087\n",
      "Bleu_4:\t0.180851574265\n",
      "METEOR:\t0.1942386256\n",
      "ROUGE_L: 0.4944843495513961\n",
      "CIDEr:\t0.5366734329784935\n",
      "model-3 saved.\n",
      "Previous epoch loss:  20537.19714164734\n",
      "Current epoch loss:  19421.903816223145\n",
      "Elapsed time:  8014.45933914\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 42968, 'guess': [43236, 39184, 35132, 31080], 'testlen': 43236, 'correct': [27565, 11003, 4376, 1837]}\n",
      "ratio: 1.00623719978\n",
      "Bleu_1:\t0.637547414192\n",
      "Bleu_2:\t0.423114020852\n",
      "Bleu_3:\t0.281468473431\n",
      "Bleu_4:\t0.190537013965\n",
      "METEOR:\t0.198521582727\n",
      "ROUGE_L: 0.501324539715839\n",
      "CIDEr:\t0.5665450666453141\n",
      "model-4 saved.\n",
      "Previous epoch loss:  19421.903816223145\n",
      "Current epoch loss:  18524.318145751953\n",
      "Elapsed time:  10028.7581501\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 42881, 'guess': [43051, 38999, 34947, 30895], 'testlen': 43051, 'correct': [27617, 10903, 4288, 1819]}\n",
      "ratio: 1.00396445978\n",
      "Bleu_1:\t0.641494971081\n",
      "Bleu_2:\t0.423489745381\n",
      "Bleu_3:\t0.280227159477\n",
      "Bleu_4:\t0.189722480144\n",
      "METEOR:\t0.200738597027\n",
      "ROUGE_L: 0.5018375338286659\n",
      "CIDEr:\t0.5851083192247601\n",
      "model-5 saved.\n",
      "Previous epoch loss:  18524.318145751953\n",
      "Current epoch loss:  17749.687211990356\n",
      "Elapsed time:  12040.9531002\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 43044, 'guess': [43423, 39371, 35319, 31267], 'testlen': 43423, 'correct': [27649, 10968, 4322, 1830]}\n",
      "ratio: 1.00880494378\n",
      "Bleu_1:\t0.636736291827\n",
      "Bleu_2:\t0.421167935543\n",
      "Bleu_3:\t0.278951684237\n",
      "Bleu_4:\t0.188793900861\n",
      "METEOR:\t0.202074947604\n",
      "ROUGE_L: 0.5000687632786457\n",
      "CIDEr:\t0.5885121640667557\n",
      "model-6 saved.\n",
      "Previous epoch loss:  17749.687211990356\n",
      "Current epoch loss:  17043.332372665405\n",
      "Elapsed time:  14056.3727131\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 43448, 'guess': [44032, 39980, 35928, 31876], 'testlen': 44032, 'correct': [27934, 11160, 4452, 1875]}\n",
      "ratio: 1.01344135518\n",
      "Bleu_1:\t0.634402252907\n",
      "Bleu_2:\t0.420816791427\n",
      "Bleu_3:\t0.279964357851\n",
      "Bleu_4:\t0.189544602643\n",
      "METEOR:\t0.202385192221\n",
      "ROUGE_L: 0.49989566999209134\n",
      "CIDEr:\t0.5840189621683802\n",
      "model-7 saved.\n",
      "Previous epoch loss:  17043.332372665405\n",
      "Current epoch loss:  16404.98136329651\n",
      "Elapsed time:  16069.3980331\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 43501, 'guess': [44106, 40054, 36002, 31950], 'testlen': 44106, 'correct': [27968, 11086, 4497, 1986]}\n",
      "ratio: 1.01390772626\n",
      "Bleu_1:\t0.63410873804\n",
      "Bleu_2:\t0.418934724317\n",
      "Bleu_3:\t0.279874282278\n",
      "Bleu_4:\t0.192131839185\n",
      "METEOR:\t0.203879064192\n",
      "ROUGE_L: 0.5001238774901829\n",
      "CIDEr:\t0.5876817580049488\n",
      "model-8 saved.\n",
      "Previous epoch loss:  16404.98136329651\n",
      "Current epoch loss:  15804.172830581665\n",
      "Elapsed time:  18083.1245842\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 43467, 'guess': [44027, 39975, 35923, 31871], 'testlen': 44027, 'correct': [27869, 10956, 4325, 1859]}\n",
      "ratio: 1.01288333678\n",
      "Bleu_1:\t0.632997933087\n",
      "Bleu_2:\t0.416517182088\n",
      "Bleu_3:\t0.275397368119\n",
      "Bleu_4:\t0.186827483485\n",
      "METEOR:\t0.20340454731\n",
      "ROUGE_L: 0.4983458284614501\n",
      "CIDEr:\t0.5807284501903027\n",
      "model-9 saved.\n",
      "Previous epoch loss:  15804.172830581665\n",
      "Current epoch loss:  15259.893159866333\n",
      "Elapsed time:  20097.86623\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 43269, 'guess': [43783, 39731, 35679, 31627], 'testlen': 43783, 'correct': [28131, 11184, 4540, 1934]}\n",
      "ratio: 1.01187917447\n",
      "Bleu_1:\t0.642509649864\n",
      "Bleu_2:\t0.425278726271\n",
      "Bleu_3:\t0.284444035905\n",
      "Bleu_4:\t0.193685559225\n",
      "METEOR:\t0.205189629439\n",
      "ROUGE_L: 0.5030913491510148\n",
      "CIDEr:\t0.6100965836906449\n",
      "model-10 saved.\n",
      "Previous epoch loss:  15259.893159866333\n",
      "Current epoch loss:  14752.233030319214\n",
      "Elapsed time:  22113.64503\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 43374, 'guess': [43842, 39790, 35738, 31686], 'testlen': 43842, 'correct': [27828, 10953, 4412, 1923]}\n",
      "ratio: 1.01078987412\n",
      "Bleu_1:\t0.634733816888\n",
      "Bleu_2:\t0.417999144322\n",
      "Bleu_3:\t0.278367597743\n",
      "Bleu_4:\t0.190213780727\n",
      "METEOR:\t0.203185010853\n",
      "ROUGE_L: 0.49878935402584107\n",
      "CIDEr:\t0.5948248563641748\n",
      "model-11 saved.\n",
      "Previous epoch loss:  14752.233030319214\n",
      "Current epoch loss:  14280.673803329468\n",
      "Elapsed time:  24127.9601641\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 43957, 'guess': [44745, 40693, 36641, 32589], 'testlen': 44745, 'correct': [28295, 11120, 4418, 1873]}\n",
      "ratio: 1.0179266101\n",
      "Bleu_1:\t0.632361157671\n",
      "Bleu_2:\t0.415695317321\n",
      "Bleu_3:\t0.275171141936\n",
      "Bleu_4:\t0.186023945075\n",
      "METEOR:\t0.203903038197\n",
      "ROUGE_L: 0.498464093112043\n",
      "CIDEr:\t0.5896411929679379\n",
      "model-12 saved.\n",
      "Previous epoch loss:  14280.673803329468\n",
      "Current epoch loss:  13844.796983718872\n",
      "Elapsed time:  26142.1917291\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 43426, 'guess': [44103, 40051, 35999, 31947], 'testlen': 44103, 'correct': [27886, 10721, 4137, 1719]}\n",
      "ratio: 1.01558973887\n",
      "Bleu_1:\t0.632292587806\n",
      "Bleu_2:\t0.411405422253\n",
      "Bleu_3:\t0.268933581487\n",
      "Bleu_4:\t0.179864405013\n",
      "METEOR:\t0.202224518459\n",
      "ROUGE_L: 0.4936245767270716\n",
      "CIDEr:\t0.5758548656481778\n",
      "model-13 saved.\n",
      "Previous epoch loss:  13844.796983718872\n",
      "Current epoch loss:  13436.943620681763\n",
      "Elapsed time:  28156.7599251\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 43815, 'guess': [44450, 40398, 36346, 32294], 'testlen': 44450, 'correct': [27759, 10599, 4137, 1772]}\n",
      "ratio: 1.01449275362\n",
      "Bleu_1:\t0.62449943757\n",
      "Bleu_2:\t0.404779527808\n",
      "Bleu_3:\t0.265188890214\n",
      "Bleu_4:\t0.178855361551\n",
      "METEOR:\t0.200042555572\n",
      "ROUGE_L: 0.49179038472400466\n",
      "CIDEr:\t0.5698480310714691\n",
      "model-14 saved.\n",
      "Previous epoch loss:  13436.943620681763\n",
      "Current epoch loss:  13074.28826713562\n",
      "Elapsed time:  30171.093935\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 43879, 'guess': [44701, 40649, 36597, 32545], 'testlen': 44701, 'correct': [28113, 10935, 4247, 1799]}\n",
      "ratio: 1.01873333485\n",
      "Bleu_1:\t0.628912104875\n",
      "Bleu_2:\t0.411319631046\n",
      "Bleu_3:\t0.26977304256\n",
      "Bleu_4:\t0.18150376341\n",
      "METEOR:\t0.20314645594\n",
      "ROUGE_L: 0.49673339725570076\n",
      "CIDEr:\t0.5766447804971495\n",
      "model-15 saved.\n",
      "Previous epoch loss:  13074.28826713562\n",
      "Current epoch loss:  12730.230701446533\n",
      "Elapsed time:  32186.1747122\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 44150, 'guess': [44972, 40920, 36868, 32816], 'testlen': 44972, 'correct': [28207, 10883, 4281, 1807]}\n",
      "ratio: 1.01861834655\n",
      "Bleu_1:\t0.62721248777\n",
      "Bleu_2:\t0.408426441329\n",
      "Bleu_3:\t0.268559869104\n",
      "Bleu_4:\t0.18071698184\n",
      "METEOR:\t0.203249723481\n",
      "ROUGE_L: 0.49568588215888404\n",
      "CIDEr:\t0.5763248447128025\n",
      "model-16 saved.\n",
      "Previous epoch loss:  12730.230701446533\n",
      "Current epoch loss:  12407.507057189941\n",
      "Elapsed time:  34202.5079551\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 43718, 'guess': [44480, 40428, 36376, 32324], 'testlen': 44480, 'correct': [27754, 10524, 4111, 1727]}\n",
      "ratio: 1.01742989158\n",
      "Bleu_1:\t0.623965827338\n",
      "Bleu_2:\t0.403022872318\n",
      "Bleu_3:\t0.263793429744\n",
      "Bleu_4:\t0.176965982134\n",
      "METEOR:\t0.199644099577\n",
      "ROUGE_L: 0.4913011311125159\n",
      "CIDEr:\t0.5664259280230383\n",
      "model-17 saved.\n",
      "Previous epoch loss:  12407.507057189941\n",
      "Current epoch loss:  12108.683952331543\n",
      "Elapsed time:  36218.1640241\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 43843, 'guess': [44593, 40541, 36489, 32437], 'testlen': 44593, 'correct': [27863, 10624, 4085, 1692]}\n",
      "ratio: 1.01710649362\n",
      "Bleu_1:\t0.624829009037\n",
      "Bleu_2:\t0.404647996761\n",
      "Bleu_3:\t0.263670552743\n",
      "Bleu_4:\t0.175847477041\n",
      "METEOR:\t0.200462994032\n",
      "ROUGE_L: 0.49108914252716823\n",
      "CIDEr:\t0.5648102262162237\n",
      "model-18 saved.\n",
      "Previous epoch loss:  12108.683952331543\n",
      "Current epoch loss:  11832.664162635803\n",
      "Elapsed time:  38235.0488961\n",
      "Saved ./data/val/val.candidate.captions.pkl..\n",
      "{'reflen': 44350, 'guess': [45283, 41231, 37179, 33127], 'testlen': 45283, 'correct': [27982, 10805, 4255, 1804]}\n",
      "ratio: 1.02103720406\n",
      "Bleu_1:\t0.617936090807\n",
      "Bleu_2:\t0.402413213008\n",
      "Bleu_3:\t0.264635909338\n",
      "Bleu_4:\t0.178237953665\n",
      "METEOR:\t0.202731930931\n",
      "ROUGE_L: 0.49093343144580526\n",
      "CIDEr:\t0.5682340185747795\n",
      "model-19 saved.\n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
